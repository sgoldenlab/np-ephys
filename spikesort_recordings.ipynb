{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f24c07c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import spikeinterface.full as si\n",
    "import matplotlib.pyplot as plt\n",
    "# import IO_tools as io\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from tools.settings import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ac439d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spikeinterface version:  0.102.3\n"
     ]
    }
   ],
   "source": [
    "from spikeinterface import __version__ as sivers\n",
    "print(f'spikeinterface version:  {sivers}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8f6b67",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4167ad26",
   "metadata": {},
   "source": [
    "#### set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b794df2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for recordings in:\n",
      "\t/mnt/array/3_TRAP_ISO/1_Recordings\n"
     ]
    }
   ],
   "source": [
    "# load config settings\n",
    "paths = settings.paths\n",
    "experiment = settings.experiment\n",
    "\n",
    "# define project paths\n",
    "raw_drive = paths.drive\n",
    "expt_folder = experiment.dir\n",
    "batch_folder = raw_drive / expt_folder / paths.data_dir\n",
    "print(f'Looking for recordings in:\\n\\t{batch_folder}')\n",
    "\n",
    "# define session paths\n",
    "raw_dir = paths.raw_dir\n",
    "processed_dir = paths.processed_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa28732d",
   "metadata": {},
   "source": [
    "set parallel processing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51826ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parallel Job parameters:\n",
      "{ 'chunk_duration': '1s',\n",
      "  'n_jobs': 6,\n",
      "  'progress_bar': True}\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "global_job_kwargs=dict(\n",
    "    n_jobs=6,\n",
    "    chunk_duration='1s',\n",
    "    progress_bar=True,\n",
    ")\n",
    "si.set_global_job_kwargs(**global_job_kwargs)\n",
    "print(\"\\nParallel Job parameters:\")\n",
    "pprint(global_job_kwargs, indent=2, width=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d325cd",
   "metadata": {},
   "source": [
    "set default kilosort parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25fe7dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check list of default values with `si.get_default_sorter_params('kilosort4')`\n",
    "ks_params = si.get_default_sorter_params('kilosort4')\n",
    "ks_params.update(dict(\n",
    "            batch_size=45000, nblocks=5, Th_universal=9.0, Th_learned=8.0,\n",
    "            save_preprocessed_copy=True, save_extra_vars=False,\n",
    "            # cluster_downsampling = 25, max_cluster_subset=None  # address potential memory issues in KS v4.1.\n",
    "        ))\n",
    "\n",
    "# session-specific sorting params\n",
    "sessions_ksparams = {\n",
    "    # 'NP01_R1': dict(\n",
    "    #     Th_universal=5.0, Th_learned=5.0, ...\n",
    "    # )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79206300",
   "metadata": {},
   "source": [
    "## sort recordings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b03e8b",
   "metadata": {},
   "source": [
    "set subject / recording pairs to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52126337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the following recordings:\n",
      "{'TRP804_R2': {'concatenate': False, 'multiple_shanks': True}}\n"
     ]
    }
   ],
   "source": [
    "recording_pairs = experiment.recordings\n",
    "print(\"Processing the following recordings:\")\n",
    "pprint(recording_pairs, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d3e19f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to other than None to compress specific raw recording (including multiple segments)\n",
    "# e.g. \"NP02_R1\", or keep as None to batch compress all recordings in recording_pairs\n",
    "recording_name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3af4f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if recording_name is not None:\n",
    "    properties = recording_pairs[recording_name]\n",
    "    print(f'---processing single recording')\n",
    "    recording_pairs = {recording_name: properties}\n",
    "# else:\n",
    "#     batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40efc9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---processing  TRP804_R2\n",
      "recording session folder:  /mnt/array/3_TRAP_ISO/1_Recordings/TRP804/TRP804_R2\n",
      "Raw folder found:\n",
      "\t/mnt/array/3_TRAP_ISO/1_Recordings/TRP804/TRP804_R2/0_raw_compressed/TRP804_R2_g0_t0\n",
      "---saving processed outputs to  \"/mnt/array/3_TRAP_ISO/1_Recordings/TRP804/TRP804_R2/2_processed\"\n",
      "Found single raw file for TRP804_R2: [PosixPath('/mnt/array/3_TRAP_ISO/1_Recordings/TRP804/TRP804_R2/0_raw_compressed/TRP804_R2_g0_t0/TRP804_R2_g0_t0.imec0.ap.cbin')]\n",
      "---processing probe 0 from file: TRP804_R2_g0_t0.imec0.ap.cbin\n"
     ]
    }
   ],
   "source": [
    "from probeinterface.plotting import plot_probe, plot_probegroup\n",
    "from torch.cuda import empty_cache\n",
    "from tools.spikesorting import load_recording, process_recording\n",
    "\n",
    "overwrite = True\n",
    "for session, properties in recording_pairs.items():\n",
    "    animal = session.split('_')[0]\n",
    "    recording_name = session\n",
    "    concatenate = properties['concatenate']\n",
    "    print(f'---processing  {recording_name}{\", multiple recordings...\" if concatenate else \"\"}')\n",
    "\n",
    "    # find session folder\n",
    "    rec_folder = batch_folder / animal / recording_name\n",
    "    if rec_folder.exists():\n",
    "        print(f'recording session folder:  {rec_folder}')  # top-level\n",
    "    else:\n",
    "        print(f'(!) No recording session folder found for:  {rec_folder}\\nSkipping...\\n\\n')\n",
    "        continue\n",
    "\n",
    "    # find raw recording folders\n",
    "    raw_folder = rec_folder / raw_dir\n",
    "    raw_folders = None\n",
    "    assert raw_folder.exists(), f'Raw folder does not exist:\\n\\t{raw_folder}'\n",
    "    if not concatenate: # single recording\n",
    "        if raw_file := next(raw_folder.glob('*.cbin'), None):  # no subfolder, raw files only\n",
    "            raw_folder = raw_file.parent\n",
    "            print(f'Raw files found in:\\n\\t{raw_folder}')\n",
    "        elif (raw_file := list(raw_folder.rglob('*.cbin'))) != []:  # subfolder with raw files\n",
    "            if len(raw_file) > 1:\n",
    "                raw_folders = [f.parent for f in raw_file]\n",
    "                print(f'Multiple raw folders found:', end='')\n",
    "                print(*raw_folders, sep='\\n\\t')\n",
    "            else:\n",
    "                raw_folder = raw_file[0].parent\n",
    "                print(f'Raw folder found:\\n\\t{raw_folder}')\n",
    "        else:\n",
    "            print(f'No recordings found for {recording_name}!\\nSkipping...\\n\\n')\n",
    "            continue\n",
    "    else:  # multiple segments for recording\n",
    "        if raw_files := raw_folder.rglob('*.cbin') != []:\n",
    "            if len(raw_files) == 1:\n",
    "                raw_folder = raw_files[0].parent\n",
    "                print(f'Only one raw file found:\\n\\t{raw_folder}')\n",
    "            else:\n",
    "                raw_folders = [f.parent for f in raw_files]\n",
    "                print(f'Raw folders found for {recording_name}:\\n\\t', end='')\n",
    "                print(*raw_folders, sep='\\n\\t')\n",
    "\n",
    "    if not raw_folders:  # single recording segment\n",
    "        raw_folders = [raw_folder]\n",
    "        rec_name = recording_name\n",
    "\n",
    "    processed_folder = rec_folder / processed_dir\n",
    "    assert processed_folder.exists(), f'Processed folder does not exist:\\n\\t{processed_folder}'\n",
    "    print(f'---saving processed outputs to  \"{processed_folder}\"')\n",
    "\n",
    "    probe_shanks = properties.get('multiple_shanks', False)\n",
    "    if not isinstance(probe_shanks, list):  # single probe\n",
    "        probe_shanks = [probe_shanks]\n",
    "\n",
    "    raw_folder = rec_folder / raw_dir\n",
    "    assert raw_folder.exists(), f\"(!) No raw data folder found for recording: {rec_folder}\\nExpected in: {raw_folder}\\nSkipping...\\n\\n\"\n",
    "    match raw_files := list(raw_folder.rglob(f'{recording_name}*imec*.cbin')):\n",
    "        case x if len(x) > 1:\n",
    "            print(f'Found multiple raw files for {recording_name}: {x}')\n",
    "        case _:\n",
    "            print(f'Found single raw file for {recording_name}: {raw_files}')\n",
    "\n",
    "    for probe_num, raw_file in enumerate(raw_files):\n",
    "        print(f'---processing probe {probe_num} from file: {raw_file.name}')\n",
    "        empty_cache()  # clear GPU memory between probes\n",
    "        # TODO: condense to spikesorting functions\n",
    "        # similar compress_recording(rec_name, rec_folder, target_folder, job_kwargs)\n",
    "        # spike_sorting(rec_name, rec_folder, job_kwargs)\n",
    "            # includes load_recording, \n",
    "\n",
    "        # load recording\n",
    "        rec = load_recording(raw_file, concatenate=concatenate) \n",
    "        if not rec:\n",
    "            print(f'(!) No valid recording found for {recording_name}!\\nSkipping...\\n\\n')\n",
    "            continue\n",
    "        \n",
    "        rec_name = f'{recording_name}_probe{probe_num}'  # with probe number\n",
    "        print(f'\\nFinal recording: {rec}\\n\\t', rec, '\\n')\n",
    "\n",
    "        # save probe channel map\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 5), dpi=300)\n",
    "        plot_probe(\n",
    "            rec.get_probe(), \n",
    "            ax=ax\n",
    "        )\n",
    "        probemap_filepath = processed_folder / f\"{rec_name}_channel_selection.png\"\n",
    "        print(f'...saving channel selection to:\\n\\t{probemap_filepath}')\n",
    "        plt.savefig(probemap_filepath, dpi=400, bbox_inches='tight')\n",
    "\n",
    "        # set session-specific sorting parameters\n",
    "        session_params = sessions_ksparams.get(recording_name, {})\n",
    "        session_params.update({'fs': rec.get_sampling_frequency()})  # add sampling frequency\n",
    "        ks_params.update(session_params)\n",
    "\n",
    "        # run sorter\n",
    "        sorter_output_folder = processed_folder / f'kilosort4_probe{probe_num}'\n",
    "        multiple_shanks = probe_shanks[probe_num] if probe_num < len(probe_shanks) else False\n",
    "        if multiple_shanks:\n",
    "            print(f'\\nRunning sorter for multiple shanks...')\n",
    "            \n",
    "            if (not overwrite) and sorter_output_folder.exists():\n",
    "                print(f'...skipping sorting for existing output:\\n\\t{sorter_output_folder}')\n",
    "                continue\n",
    "            print('\\n---------Starting sorting---------\\n')\n",
    "            print(f'...saving sorting output to:\\n\\t{sorter_output_folder}')        \n",
    "            aggregated_sorting = si.run_sorter_by_property(\n",
    "                sorter_name='kilosort4',\n",
    "                recording=rec,\n",
    "                grouping_property='group',  # defines shank number in recording object\n",
    "                folder=sorter_output_folder,\n",
    "                remove_existing_folder=True,\n",
    "                docker_image=False,\n",
    "                **ks_params\n",
    "            )\n",
    "            # concatenate shank sortings into one folder for postprocessing\n",
    "            try:\n",
    "                aggregated_sorting.save_to_zarr(sorter_output_folder / 'aggregated_sorting', overwrite=True, **global_job_kwargs)\n",
    "            except:\n",
    "                aggregated_sorting.save_to_zarr(sorter_output_folder / 'aggregated_sorting', overwrite=True)\n",
    "            print('\\n---------Sorting finished---------\\n\\n')\n",
    "\n",
    "        else:\n",
    "            print(f'\\nRunning sorter for single shank...')\n",
    "            print('\\n---------Starting sorting---------\\n')\n",
    "            print(f'...saving sorting output to:\\n\\t{sorter_output_folder}')\n",
    "            sorting = si.run_sorter(\n",
    "                sorter_name='kilosort4',\n",
    "                recording=rec,\n",
    "                folder=sorter_output_folder,\n",
    "                remove_existing_folder=True,\n",
    "                docker_image=False,\n",
    "                verbose=True,\n",
    "                **ks_params\n",
    "            )\n",
    "            # save separate sorting object for postprocessing\n",
    "            sorting.save_to_zarr(sorter_output_folder / 'sorting', overwrite=True)\n",
    "            print('\\n---------Sorting finished---------\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "np-ephys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
